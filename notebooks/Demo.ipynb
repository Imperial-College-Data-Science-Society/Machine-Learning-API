{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ICDSS Machine Learning Workshop Series: Coding Models on `scikit-learn`, `keras` & `fbprophet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Pipeline](#pipeline)\n",
    "    * [Preprocessing](#pipe:preprocessing)\n",
    "    * [Estimation](#pipe:estimation)\n",
    "        * [Supervised Learning](#pipe:supervised-learning)\n",
    "        * [Unsupervised Learning](#pipe:unsupervised-learning)\n",
    "    * [Evaluation](#pipe:evaluation)\n",
    "* [`scikit-learn`](#scikit-learn)\n",
    "    * [Preprocessing](#sk:preprocessing)\n",
    "    * [Estimation](#sk:estimation)\n",
    "    * [Model Selection](#sk:model-selection)\n",
    "        * [Hyperparameters](#sk:hyperparameters)\n",
    "        * [`GridSearchCV`](#sk:GridSearchCV)\n",
    "    * [Pipeline](#sk:pipeline)\n",
    "* [`keras`](#keras)\n",
    "    * [Dense](#keras:dense)\n",
    "        * [Iris](#keras:iris)\n",
    "    * [CNN](#keras:cnn)\n",
    "        * [Fashion MNIST](#keras:fashion-mnist)\n",
    "    * [LSTM](#keras:lstm)\n",
    "        * [President Trump Generator](#keras:president-trump-generator)\n",
    "* [`fbprophet`](#fbprophet)\n",
    "    * [Bitcoin Capital Market](#fbprophet:bitcoin-capital-market)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline <a class=\"anchor\" id=\"pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Pipeline.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-muted')\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing <a class=\"anchor\" id=\"pipe:preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"Generic `preprocessing` transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, **hyperparameters):\n",
    "        \"\"\"Initialise Hyperparameters\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self, X_train):\n",
    "        \"\"\"Set state of `preprocessor`.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply transformation.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Reset state and apply transformer.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principle Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"Principle Component Analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=2):\n",
    "        \"\"\"Contructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_comps: int\n",
    "            Number of principle components\n",
    "        \"\"\"\n",
    "        self.n_comps = n_components\n",
    "        self.mu = None\n",
    "        self.U = None\n",
    "        self._fitted = False\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit PCA according to `X.cov()`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Features matrix\n",
    "        Returns\n",
    "        -------\n",
    "        array: numpy.ndarray\n",
    "            Transformed features matrix\n",
    "        \"\"\"\n",
    "        self.D, N = X.shape\n",
    "        self.mu = X.mean(axis=1).reshape(-1, 1)\n",
    "        # center data\n",
    "        A = X - self.mu\n",
    "        # covariance matrix\n",
    "        S = (1 / N) * np.dot(A.T, A)\n",
    "        # eigendecomposition\n",
    "        _l, _v = np.linalg.eig(S)\n",
    "        _l = np.real(_l)\n",
    "        _v = np.real(_v)\n",
    "        # short eigenvalues\n",
    "        _indexes = np.argsort(_l)[::-1]\n",
    "        # sorted eigenvalues and eigenvectors\n",
    "        l, v = _l[_indexes], _v[:, _indexes]\n",
    "        # eigenvalues\n",
    "        V = v[:, :self.n_comps]\n",
    "        # unnormalised transformer\n",
    "        _U = np.dot(A, V)\n",
    "        # transformation matrix\n",
    "        self.U = _U / np.apply_along_axis(np.linalg.norm, 0, _U)\n",
    "        # unnormalised transformed features\n",
    "        W = np.dot(self.U.T, A)\n",
    "        # data statistics\n",
    "        self.W_mu = np.mean(W, axis=1)\n",
    "        self.W_std = np.std(W, axis=1)\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform `X` by projecting it to PCA feature space.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Features matrix\n",
    "        Returns\n",
    "        -------\n",
    "        array: numpy.ndarray\n",
    "            Transformed features matrix\n",
    "        \"\"\"\n",
    "        if not self._fitted:\n",
    "            raise AssertionError('Not fitted yet.')\n",
    "        # centered data\n",
    "        Phi = X - self.mu\n",
    "        # unnormalised transformed features\n",
    "        W = np.dot(self.U.T, Phi)\n",
    "\n",
    "        return ((W.T - self.W_mu) / self.W_std).T\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit PCA according to `X.cov()`\n",
    "        and then transform `X` by\n",
    "        projecting it to PCA feature space.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Features matrix\n",
    "        Returns\n",
    "        -------\n",
    "        array: numpy.ndarray\n",
    "            Transformed features matrix\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fetch data\n",
    "iris = datasets.load_iris()\n",
    "# supervised-learning data\n",
    "X, y = iris.data, iris.target\n",
    "# transformed data\n",
    "X_transform = pca.fit_transform(X.T)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(20.0, 3.0))\n",
    "for i in range(iris.target.max() + 1):\n",
    "    I = iris.target == i\n",
    "    ax.scatter(X_transform[0, I], X_transform[1, I], label=iris.target_names[i])\n",
    "    ax.set_title('Iris Dataset PCA')\n",
    "    ax.set_xlabel('Principle Component 1')\n",
    "    ax.set_ylabel('Principle Component 2')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `challenge` QuadraticFeatures\n",
    "\n",
    "Code the transformation $\\mathcal{G}$ such that:\n",
    "\n",
    "$$\\mathcal{G}: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}^{6}$$\n",
    "\n",
    "according to the mapping:\n",
    "\n",
    "$$\\begin{bmatrix}x_{1} & x_{2}\\end{bmatrix} \\mapsto \\begin{bmatrix}1 & x_{1} & x_{2} & x_{1}x_{2} & x_{1}^{2} & x_{2}^{2}\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticFeatures:\n",
    "    \"\"\"Generate Quadratic features.\"\"\"\n",
    "    \n",
    "    def fit(self, X_train):\n",
    "        \"\"\"Set state of `preprocessor`.\"\"\"\n",
    "        pass\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply transformation.\"\"\"\n",
    "        # get dimensions\n",
    "        N, D = X.shape\n",
    "        # check number of input features\n",
    "        assert(D==2)\n",
    "        \n",
    "        # get x_{1} column\n",
    "        x_1 = X[:, 0]\n",
    "        # get x_{2} column\n",
    "        x_2 = X[:, 1]\n",
    "        \n",
    "        # initialise output matrix\n",
    "        out = np.empty(shape=(N, 6))\n",
    "        # column 1: constant\n",
    "        out[:, 0] = \n",
    "        # column 2: x_{1}\n",
    "        out[:, 1] = \n",
    "        # column 3: x_{2}\n",
    "        out[:, 2] = \n",
    "        # column 4: x_{1}x_{2}\n",
    "        out[:, 3] = \n",
    "        # column 5: x_{1}^{2}\n",
    "        out[:, 4] = \n",
    "        # column 6: x_{2}^{2}\n",
    "        out[:, 5] = \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Reset state and apply transformer.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unit Test for `QuadraticFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test function\n",
    "assert (QuadraticFeatures().fit_transform(np.array([[1, 2],\n",
    "                                                    [3, 4]])) == np.array([[ 1.,  1.,  2.,  2.,  1.,  4.],\n",
    "                                                                           [ 1.,  3.,  4., 12.,  9., 16.]])).all(), \"Wrong implementation, try again!\"\n",
    "'Well Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation <a class=\"anchor\" id=\"pipe:estimation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    \"\"\"Generic `estimator` class.\"\"\"\n",
    "    \n",
    "    def __init__(self, **hyperparameters):\n",
    "        \"\"\"Initialise Hyperparameters\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train model.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Forward/Inference pass.\"\"\"\n",
    "        return y_hat\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Performance results.\"\"\"\n",
    "        y_hat = self.predict(X)\n",
    "        return self._loss(y, y_hat)\n",
    "    \n",
    "    def _loss(self, y, y_hat):\n",
    "        \"\"\"Objective function for scoring.\"\"\"\n",
    "        return L(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `challenge` Linear Regression\n",
    "\n",
    "Code the estimator $\\mathcal{F}$ such that:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X} * \\mathbf{w}_{MLE}$$\n",
    "\n",
    "for the Maximum Likelihood estimation weights parameters:\n",
    "\n",
    "$$\\mathbf{w}_{MLE} = (\\mathbf{X}^{T} \\mathbf{X})^{-1} * \\mathbf{X}^{T} * \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"Linear Regression `estimator` class.\"\"\"\n",
    "    \n",
    "    def __init__(self, dimensionality):\n",
    "        \"\"\"Initialise Hyperparameters\"\"\"\n",
    "        self.dimensionality = \n",
    "        self.w_mle = \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train model.\"\"\"\n",
    "        self.w_mle = \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Forward/Inference pass.\"\"\"\n",
    "        y_hat = \n",
    "        return y_hat\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Performance results.\"\"\"\n",
    "        y_hat = self.predict(X)\n",
    "        return self._loss(y, y_hat)\n",
    "    \n",
    "    def _loss(self, y, y_hat):\n",
    "        \"\"\"Objective function for scoring.\"\"\"\n",
    "        return ((y - y_hat) ** 2).mean(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unit Test for `LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data\n",
    "X, y = np.array([[1., 2.], [1., 3.], [1., 7.]]), np.array([2., 3., 7.])\n",
    "# input dimensions\n",
    "N, D = X.shape\n",
    "# estimator: init & fit\n",
    "lr = LinearRegression(dimensionality=D).fit(X, y)\n",
    "\n",
    "# unit test function\n",
    "assert np.isclose(lr.predict(X), y).all(), 'Wrong implementation, try again!'\n",
    "'Well Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scikit-learn` <a class=\"anchor\" id=\"scikit-learn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing <a class=\"anchor\" id=\"sk:preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.decomposition\n",
    "import sklearn.pipeline\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-muted')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principle Component Analysis <a class=\"anchor\" id=\"sk:pca\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "\n",
    "# fetch data\n",
    "iris = datasets.load_iris()\n",
    "# supervised-learning data\n",
    "X, y = iris.data, iris.target\n",
    "# transformed data\n",
    "X_transform = pca.fit_transform(X)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(20.0, 3.0))\n",
    "for i in range(iris.target.max() + 1):\n",
    "    I = iris.target == i\n",
    "    ax.scatter(X_transform[I, 0], X_transform[I, 1], label=iris.target_names[i])\n",
    "    ax.set_title('Iris Dataset PCA')\n",
    "    ax.set_xlabel('Principle Component 1')\n",
    "    ax.set_ylabel('Principle Component 2')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation <a class=\"anchor\" id=\"sk:estimation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/ml_map.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Let's try to model a continous function:\n",
    "\n",
    "$$f(x) = x^{3} - 0.4x^{2} - x + 0.3 + \\epsilon, \\quad x \\in [-1, 1] \\text{ and } \\epsilon \\sim \\mathcal{N}(0, 0.05)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "x = np.linspace(-1, 1, 500)\n",
    "# targets\n",
    "y = (x**3 - 0.4*x**2 - x + 0.3) + np.random.normal(0, 0.01, len(x))\n",
    "# features matrix\n",
    "X = x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "# true data\n",
    "ax.plot(x, y, label='Observations', lw=2)\n",
    "ax.fill_between(x, y-0.025, y+0.025)\n",
    "\n",
    "## (1) INIT - set hyperparameters\n",
    "estimators = [('Gradient Boosting Tree Regressor', sklearn.ensemble.GradientBoostingRegressor(n_estimators=25)),\n",
    "              ('Support Vector Machine Regressor', sklearn.svm.SVR(C=1.0, kernel='rbf')),\n",
    "              ('Ridge Regressor', sklearn.linear_model.Ridge(alpha=0.5)),\n",
    "              ('K-Nearest Neighbors Regressor', sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)),\n",
    "              ('Multi-Layer Perceptron Regressor', sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(15,),\n",
    "                                                                                       activation='relu'))\n",
    "             ]\n",
    "\n",
    "for name, model in estimators:\n",
    "    ## (2) FIT - train model\n",
    "    model.fit(X, y)\n",
    "    ## (3) PREDICT - make predictions\n",
    "    y_hat = model.predict(X)\n",
    "    ## (4) SCORE - evaluate model\n",
    "    score = model.score(X, y)\n",
    "    \n",
    "    print('[Score] %s: %.3f' % (name, score))\n",
    "    # figure settings\n",
    "    ax.plot(x, y_hat, label=name, lw=3)\n",
    "\n",
    "ax.set_title('Estimators Comparison Matrix')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Features\n",
    "\n",
    "$f(x)$ is a highly non-linear function (cubic) and therefore it cannot be adequately modelled by a linear estimator,\n",
    "nonetheless we will fit a `Linear SVM Regressor` and ascess its performance (visually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "# true data\n",
    "ax.plot(x, y, lw=3, label='Observations')\n",
    "\n",
    "## (1) INIT\n",
    "svr = sklearn.svm.SVR(C=1.0, kernel='linear')\n",
    "## (2) FIT\n",
    "svr.fit(X, y)\n",
    "## (3) PREDICT\n",
    "y_hat = svr.predict(X)\n",
    "## (4) SCORE\n",
    "score = svr.score(X, y)\n",
    "print('[Score] %s: %.3f' % ('Linear SVM', score))\n",
    "\n",
    "# figure settings\n",
    "ax.plot(x, y_hat, label='Model')\n",
    "ax.set_title('Linear SVM Regressor')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cubic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=3)\n",
    "# generate cubic features\n",
    "X_transform = poly.fit_transform(X)\n",
    "\n",
    "##### REPEAT THE SAME, X -> X_transform #####\n",
    "\n",
    "# initialize figure\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "# true data\n",
    "ax.plot(x, y, lw=3, label='Observations')\n",
    "\n",
    "## (1) INIT\n",
    "svr = sklearn.svm.SVR(C=1.0, kernel='linear')\n",
    "## (2) FIT\n",
    "svr.fit(X_transform, y) # X -> X_transform #\n",
    "## (3) PREDICT\n",
    "y_hat = svr.predict(X_transform)\n",
    "## (4) SCORE\n",
    "score = svr.score(X_transform, y)\n",
    "print('[Score] %s: %.3f' % ('Linear SVM with Cubic Features', score))\n",
    "\n",
    "# figure settings\n",
    "ax.plot(x, y_hat, label='Model')\n",
    "ax.set_title('Linear SVM Regressor with Cubic Features')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `challenge` AdaBoost Regressor\n",
    "\n",
    "Model the continuous function using the Boosting Regressor `AdaBoostRegressor`:\n",
    "\n",
    "$$g(x) = -0.3x^{7} + 7x^{2} + \\epsilon, \\quad x \\in [-0.5, 1] \\text{ and } \\epsilon \\sim \\mathcal{N}(0, 0.5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "x_a = np.linspace(-0.5, 1, 500)\n",
    "# targets\n",
    "y_a = (-0.3*x_a**7 + 7*x_a**2) + np.random.normal(0, 0.5, len(x_a))\n",
    "# features matrix\n",
    "X_a = x_a.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=7)\n",
    "# generate cubic features\n",
    "X_transform_a = poly.fit_transform(X_a)\n",
    "\n",
    "# initialize figure\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "# true data\n",
    "ax.plot(x_a, y_a, label='Observations')\n",
    "\n",
    "## (1) INIT\n",
    "ada = \n",
    "## (2) FIT\n",
    "\n",
    "## (3) PREDICT\n",
    "y_hat = \n",
    "## (4) SCORE\n",
    "score = \n",
    "print('[Score] %s: %.3f' % ('AdaBoostRegressor with 7th Order Polynomial Features', score))\n",
    "\n",
    "# figure settings\n",
    "ax.plot(x_a, y_hat, lw=3, label='Model')\n",
    "ax.set_title('AdaBoostRegressor with 7th Order Polynomial Features')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unit Test for `AdaBoostRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test function\n",
    "assert score > 0.9, 'Wrong implementation, low score!! Try again!'\n",
    "'Well Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection <a class=\"anchor\" id=\"sk:model-selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters <a class=\"anchor\" id=\"sk:hyperparameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "# true data\n",
    "ax.plot(x, y, label='original', lw=3)\n",
    "\n",
    "# parameters\n",
    "n_estimators_params = [1, 2, 10, 25]\n",
    "\n",
    "for n_estimators in n_estimators_params:\n",
    "    # initialize estimator\n",
    "    gbr = sklearn.ensemble.GradientBoostingRegressor(n_estimators=n_estimators).fit(X, y)\n",
    "    # prediction\n",
    "    y_hat = gbr.predict(X)\n",
    "    \n",
    "    # figure settings\n",
    "    ax.plot(x, y_hat, label='n_estimators=%s' % n_estimators)\n",
    "    ax.set_title('Gradient Boosting Tree Regressor')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=3)\n",
    "# generate cubic features\n",
    "X_transform = poly.fit_transform(X)\n",
    "\n",
    "# initialize figure\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "# true data\n",
    "ax.plot(x, y, label='original', lw=3)\n",
    "\n",
    "# parameters\n",
    "c_params = [1.0, 0.001]\n",
    "\n",
    "for c in c_params:\n",
    "    # initialize estimator\n",
    "    svr = sklearn.svm.SVR(C=c, kernel='linear').fit(X_transform, y)\n",
    "    # prediction\n",
    "    y_hat = svr.predict(X_transform)\n",
    "    \n",
    "    # figure settings\n",
    "    ax.plot(x, y_hat, label='c=%s' % c)\n",
    "    ax.set_title('SVM Regressor')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `GridSearchCV` <a class=\"anchor\" id=\"sk:GridSearchCV\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "digits = datasets.load_digits()\n",
    "X_train_raw, X_test_raw, y_train, y_test = sklearn.model_selection.train_test_split(digits.data,\n",
    "                                                                                    digits.target,\n",
    "                                                                                    test_size=0.25)\n",
    "# use PCA to reduce input dimensionality\n",
    "pca = sklearn.decomposition.PCA(n_components=10)\n",
    "X_train = pca.fit_transform(X_train_raw)\n",
    "X_test = pca.transform(X_test_raw)\n",
    "\n",
    "# estimator hyperparameters grid\n",
    "param_grid = {'n_estimators': [1, 5, 10, 25, 50, 75, 100, 200],\n",
    "              'max_depth': [5, 7, 11, 15, 20, 45]\n",
    "             }\n",
    "\n",
    "## (1) INIT - set hyperparameters **RANGES**, not single values\n",
    "search = sklearn.model_selection.GridSearchCV(sklearn.ensemble.RandomForestClassifier(), param_grid)\n",
    "\n",
    "## (2) FIT\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "## (**) REPORT - cross validation results\n",
    "results = pd.DataFrame(search.cv_results_).set_index(['param_' + key for key in param_grid.keys()])\n",
    "mean_test_score = results['mean_test_score'].unstack(0)\n",
    "# figure settings\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "sns.heatmap(mean_test_score, annot=True, cmap=plt.cm.Reds, ax=ax)\n",
    "ax.set_title('Cross-Validation Accuracy')\n",
    "\n",
    "## (**) SELECT - pick the best model\n",
    "model = search.best_estimator_\n",
    "print('[Select] Best parameters: %s' % search.best_params_)\n",
    "\n",
    "## (3) PREDICT\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "## (4) SCORE\n",
    "score = model.score(X_test, y_test)\n",
    "print('[Score] %s: %.3f' % ('Best Random Forest Classifier', score))\n",
    "\n",
    "# confusion matrix\n",
    "cm = sklearn.metrics.confusion_matrix(y_test, y_hat)\n",
    "\n",
    "# figure settings\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "sns.heatmap(cm, ax=ax, annot=True, cmap=plt.cm.Blues)\n",
    "ax.set_title('Random Forest Classifier')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline <a class=\"anchor\" id=\"sk:pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.pipeline.Pipeline` is a container that put all the pieces:\n",
    "\n",
    "1. Preprocessing\n",
    "1. Estimation\n",
    "1. Model Selection\n",
    "\n",
    "together, using the common `fit`-`predict`-`score` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(digits.data,\n",
    "                                                                            digits.target,\n",
    "                                                                            test_size=0.25)\n",
    "\n",
    "# data flow\n",
    "steps = [('pca', sklearn.decomposition.PCA(n_components=2)),\n",
    "         ('rf', sklearn.ensemble.RandomForestClassifier())]\n",
    "\n",
    "# estimator hyperparameters grid\n",
    "# prepend the name of the step that the parameter corresponds to\n",
    "# (i.e 'pca' or 'rf') followed by two underscores '__'\n",
    "param_grid = {'pca__n_components': [5, 10, 20, 35, 45, 64],\n",
    "              'rf__n_estimators': [1, 25, 50, 75, 100, 200],\n",
    "              'rf__max_depth': [5, 11, 15, 20, 45]\n",
    "             }\n",
    "\n",
    "## (1) INIT\n",
    "# pipeline\n",
    "pipe = sklearn.pipeline.Pipeline(steps=steps)\n",
    "# grid-search\n",
    "search = sklearn.model_selection.GridSearchCV(pipe, param_grid)\n",
    "\n",
    "## (2) FIT\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "## (**) SELECT - pick the best model\n",
    "model = search.best_estimator_\n",
    "print('[Select] Best parameters: %s' % search.best_params_)\n",
    "\n",
    "## (3) PREDICT\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "## (4) SCORE\n",
    "score = model.score(X_test, y_test)\n",
    "print('[Score] %s: %.3f' % ('Best Random Forest Classifier', score))\n",
    "\n",
    "# confusion matrix\n",
    "cm = sklearn.metrics.confusion_matrix(y_test, y_hat)\n",
    "\n",
    "# figure settings\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "sns.heatmap(cm, ax=ax, annot=True, cmap=plt.cm.Blues)\n",
    "ax.set_title('Pipeline of PCA and Random Forest Classifier')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `challenge` Naive [`tpot`](https://github.com/EpistasisLab/tpot) Framework\n",
    "\n",
    "Code a `pipeline` that optimizes both the estimator type and its hyperparameters on the raw digits dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `keras` <a class=\"anchor\" id=\"keras\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Dataset <a class=\"anchor\" id=\"keras:iris\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# split to train/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)\n",
    "# one-hot encode categorical output\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# matrix shapes\n",
    "N_train, D = X_train.shape\n",
    "N_test, _ = X_test.shape\n",
    "_, M = y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "X = Input(shape=(D,), name=\"X\")\n",
    "\n",
    "# Convolution Layer\n",
    "A1 = Dense(16, name=\"A1\")(X)\n",
    "# Non-Linearity\n",
    "Z1 = Activation(\"relu\", name=\"Z1\")(A1)\n",
    "# Affine Layer\n",
    "A2 = Dense(M, name=\"A2\")(Z1)\n",
    "# Multi-Class Classification\n",
    "Y = Activation(\"softmax\", name=\"Y\")(A2)\n",
    "\n",
    "# Define Graph\n",
    "model = Model(inputs=X, outputs=Y)\n",
    "# Compile Graph\n",
    "model.compile(optimizer=RMSprop(lr=0.04),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Computational Graph Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "history = model.fit(X_train, y_train_one_hot, epochs=100, validation_split=0.25, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_one_hot = model.predict(X_test)\n",
    "\n",
    "# one-hot-encoded to raw data\n",
    "y_hat = np.argmax(y_hat_one_hot, axis=1)\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "_, ax = plt.subplots(figsize=(7.0, 6.0))\n",
    "sns.heatmap(cm_norm, annot=True, cmap=plt.cm.Blues, ax=ax)\n",
    "ax.set_xticklabels(iris.target_names, rotation=45)\n",
    "ax.set_yticklabels(iris.target_names, rotation=45)\n",
    "ax.set_title('Iris Dataset Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Class')\n",
    "ax.set_ylabel('True Class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST <a class=\"anchor\" id=\"keras:fashion-mnist\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# fetch data\n",
    "(X_train_raw, y_train), (X_test_raw, y_test) = pickle.load(open('data/fashion-mnist/fashion-mnist.pkl', 'rb'))\n",
    "## ORIGINAL IMPLEMNTATION\n",
    "# (X_train_raw, y_train), (X_test_raw, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# one-hot encode categorical output\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# tensor shape\n",
    "N_train, h, w = X_train_raw.shape\n",
    "N_test, _, _ = X_test_raw.shape\n",
    "_, M = y_train_one_hot.shape\n",
    "\n",
    "# convert raw pixels to tensors\n",
    "X_train = X_train_raw.reshape(-1, h, w, 1)\n",
    "X_test = X_test_raw.reshape(-1, h, w, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer: shape=(height, width, number of channels)\n",
    "X = Input(shape=(h, w, 1), name=\"X\")\n",
    "\n",
    "# Convolution Layer\n",
    "CONV1 = Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", name=\"CONV1\")(X)\n",
    "# Max Pooling Layer\n",
    "POOL1 = MaxPooling2D(pool_size=(2, 2), name=\"POOL1\")(CONV1)\n",
    "# Convolution Layer\n",
    "CONV2 = Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", name=\"CONV2\")(POOL1)\n",
    "# Max Pooling Layer\n",
    "POOL2 = MaxPooling2D(pool_size=(2, 2), name=\"POOL2\")(CONV2)\n",
    "# Convolution Layer\n",
    "CONV3 = Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", name=\"CONV3\")(POOL2)\n",
    "# Max Pooling Layer\n",
    "POOL3 = MaxPooling2D(pool_size=(2, 2), name=\"POOL3\")(CONV3)\n",
    "# Convert 3D feature map to 1D\n",
    "FLAT = Flatten()(POOL3)\n",
    "# Fully Connected Layer\n",
    "FC1 = Dense(units=64, name=\"FC1\")(FLAT)\n",
    "# Dropout\n",
    "DROP = Dropout(rate=0.5, name=\"DROP\")(FC1)\n",
    "# Multi-Class Classification Output Layer\n",
    "Y = Dense(M, activation=\"softmax\", name=\"Y\")(DROP)\n",
    "\n",
    "# Define Graph\n",
    "model = Model(inputs=X, outputs=Y)\n",
    "# Compile Graph\n",
    "model.compile(optimizer=RMSprop(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Computational Graph Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "history = model.fit(X_train, y_train_one_hot, batch_size=128, epochs=3, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_one_hot = model.predict(X_test)\n",
    "\n",
    "# one-hot-encoded to raw data\n",
    "y_hat = np.argmax(y_hat_one_hot, axis=1)\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "_, ax = plt.subplots(figsize=(20.0, 6.0))\n",
    "sns.heatmap(cm_norm, annot=True, cmap=plt.cm.Greens, ax=ax)\n",
    "ax.set_title('Fashion MNIST Dataset Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Class')\n",
    "ax.set_ylabel('True Class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### President Trump Generator <a class=\"anchor\" id=\"keras:president-trump-generator\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "with open(\"data/trump/speeches.txt\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "print(\"Loaded a corpus of {0} characters\".format(len(corpus)))\n",
    "corpus_length = len(corpus)\n",
    "\n",
    "# Get a unique identifier for each char in the corpus\n",
    "# then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))\n",
    "\n",
    "# it slices, it dices, it makes julienned datasets!\n",
    "# chop up our data into X and y, slice into roughly (num_chars / skip) overlapping 'sentences'\n",
    "# of length sentence_length, and encode the chars\n",
    "sentence_length = 50\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))\n",
    "\n",
    "# Vectorize our data and labels. We want everything in one-hot\n",
    "# because smart data encoding cultivates phronesis and virtue.\n",
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=np.bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1\n",
    "\n",
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\".format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\".format(X.shape, sentence_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=256, input_shape=(sentence_length, num_chars), name=\"LSTM\"))\n",
    "model.add(Dense(units=num_chars, activation=\"softmax\", name=\"Y\"))\n",
    "\n",
    "# Compile Graph\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy')\n",
    "# Computational Graph Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this line to train the model again\n",
    "if None:\n",
    "    # Dump our model architecture to a file so we can load it elsewhere\n",
    "    architecture = model.to_yaml()\n",
    "    with open('models/trump/architecture.yaml', 'a') as model_file:\n",
    "        model_file.write(architecture)\n",
    "\n",
    "    # Set up checkpoints\n",
    "    file_path=\"models/trump/weights-{epoch:02d}-{loss:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "    # model training\n",
    "    history = model.fit(X, y, epochs=30, batch_size=256, callbacks=callbacks)\n",
    "else:\n",
    "    # load weights from checkpoint\n",
    "    model.load_weights(\"models/trump/weights-03-2.152.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seed_pattern):\n",
    "    X = np.zeros((1, sentence_length, num_chars), dtype=np.bool)\n",
    "    for i, character in enumerate(seed_pattern):\n",
    "        X[0, i, encoding[character]] = 1\n",
    "\n",
    "    generated_text = \"\"\n",
    "    for i in range(10):\n",
    "        prediction = np.argmax(model.predict(X, verbose=0))\n",
    "\n",
    "        generated_text += decoding[prediction]\n",
    "\n",
    "        activations = np.zeros((1, 1, num_chars), dtype=np.bool)\n",
    "        activations[0, 0, prediction] = 1\n",
    "        X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def make_seed(seed_phrase=\"\"):\n",
    "    if seed_phrase:\n",
    "        phrase_length = len(seed_phrase)\n",
    "        pattern = \"\"\n",
    "        for i in range (0, sentence_length):\n",
    "            pattern += seed_phrase[i % phrase_length]\n",
    "    else:\n",
    "        seed = randint(0, corpus_length - sentence_length)\n",
    "        pattern = corpus[seed:seed + sentence_length]\n",
    "\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed letter\n",
    "seed = \"b\"\n",
    "# prediction\n",
    "seed + generate(make_seed(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fbprophet` <a class=\"anchor\" id=\"fbprophet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# fetch data\n",
    "btc = pickle.load(open('data/bitcoin/btc.pkl', 'rb'))\n",
    "## ORIGINAL IMPLEMNTATION\n",
    "# btc = web.DataReader(\"BCHARTS/KRAKENUSD\", data_source=\"quandl\").dropna()\n",
    "\n",
    "# time-series DataFrame\n",
    "df = pd.DataFrame(columns=[\"ds\", \"y\"])\n",
    "df[\"ds\"] = btc.index\n",
    "# market capital column\n",
    "df[\"y\"] = (btc[\"Close\"] * btc[\"VolumeBTC\"]).values\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP Optimization and Hamiltonian Monte Carlo Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Prophet(daily_seasonality=True)\n",
    "\n",
    "# train model\n",
    "model.fit(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "future = model.make_future_dataframe(periods=365)\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# generate plots\n",
    "model.plot(forecast);\n",
    "model.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "Presentations are intended for educational purposes only and do not replace independent professional judgment.\n",
    "Statements of fact and opinions expressed are those of the participants individually and,\n",
    "unless expressly stated to the contrary, are not the opinion or position of the ICDSS, its cosponsors, or its committees.\n",
    "The ICDSS does not endorse or approve, and assumes no responsibility for, the content,\n",
    "accuracy or completeness of the information presented.\n",
    "Attendees should note that sessions are video-recorded and may be published in various media, including print,\n",
    "audio and video formats without further notice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
